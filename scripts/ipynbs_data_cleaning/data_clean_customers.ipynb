{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6q4xczZt4Sa",
        "outputId": "767b3bad-d61e-450e-99c8-87c23803f7cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=3dad8aa21fbc9a1681b1bfd3c44d2e6ead715ba599fedc4d8c2b7af1fae4b19c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H1R1b6F1t9tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tempViews\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"PandasToPySparkWithSchema\").getOrCreate()\n",
        "\n",
        "# Define the schema so it matches with the Big Query later\n",
        "# NOTE: Please adjust the column names and types according to your CSV's columns.\n",
        "schema = StructType([\n",
        "    StructField(\"CustomerID\", IntegerType(), nullable=False),\n",
        "    StructField(\"PersonID\", IntegerType(), nullable=True),\n",
        "    StructField(\"StoreID\", FloatType(), nullable=True),\n",
        "    StructField(\"TerritoryID\", IntegerType(), nullable=True),\n",
        "    StructField(\"AccountNumber\", StringType(), nullable=True),\n",
        "    StructField(\"rowguid\", StringType(), nullable=True),\n",
        "    StructField(\"ModifiedDate\", TimestampType(), nullable=True)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "df_customer = spark.read.csv('table_customer.csv', sep=',', encoding='utf-8', header=True, schema=schema)\n",
        "df_person = spark.read.csv('table_person.csv', sep=',', encoding='utf-8', header=True)\n",
        "#--migue person\n",
        "#df_person_2 = spark.read.csv('migue_Person.csv', sep=',', encoding='utf-8', header=True)\n",
        "df_soh = spark.read.csv('table_soh_new.csv', sep=',', encoding='utf-8', header=True)\n",
        "\n",
        "\n",
        "df_customer.createOrReplaceTempView(\"customer\")\n",
        "\n",
        "df_person.createOrReplaceTempView(\"person\")\n",
        "\n",
        "df_soh.createOrReplaceTempView(\"soh\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_personid_customer = spark.sql(\"\"\"\n",
        "\n",
        "    SELECT c.CustomerID,\n",
        "           soh.CustomerID AS PersonID,\n",
        "           c.StoreID,\n",
        "           c.TerritoryID,\n",
        "           c.AccountNumber,\n",
        "           c.rowguid,\n",
        "           c.ModifiedDate\n",
        "\n",
        "    FROM customer AS c\n",
        "    Inner JOIN  soh\n",
        "      ON c.CustomerID = soh.CustomerID\n",
        "\n",
        "\n",
        "\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "m3RovZLPt-CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_personid_customer = df_personid_customer.select(\n",
        "    col(\"CustomerID\").cast(IntegerType()).alias(\"CustomerID\"),\n",
        "    col(\"PersonID\").cast(IntegerType()).alias(\"PersonID\"),\n",
        "    col(\"StoreID\").cast(FloatType()).alias(\"StoreID\"),\n",
        "    col(\"TerritoryID\").cast(IntegerType()).alias(\"TerritoryID\"),\n",
        "    col(\"AccountNumber\").cast(StringType()).alias(\"AccountNumber\"),\n",
        "    col(\"rowguid\").cast(TimestampType()).alias(\"rowguid\"),\n",
        ")\n",
        "df_personid_customer.printSchema()\n",
        "#df_personid_customer.write.csv('table_customer_person_id.csv', header=True, mode='overwrite')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c7mfLpwt9_4",
        "outputId": "a78653c2-4977-4354-cf51-3a8873b5def4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- PersonID: integer (nullable = true)\n",
            " |-- StoreID: float (nullable = true)\n",
            " |-- TerritoryID: integer (nullable = true)\n",
            " |-- AccountNumber: string (nullable = true)\n",
            " |-- rowguid: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DataViz new table customer test\n",
        "\n",
        "df_new = spark.read.csv('table_customer_personid_new.csv', sep=',', encoding='utf-8', header=True)"
      ],
      "metadata": {
        "id": "GzAmh1yUvbf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.printSchema()\n",
        "print()\n",
        "df_new.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZVjhlPNvyAs",
        "outputId": "42f190e8-e4db-479f-d26c-ea3dc9f95e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CustomerID: string (nullable = true)\n",
            " |-- PersonID: string (nullable = true)\n",
            " |-- StoreID: string (nullable = true)\n",
            " |-- TerritoryID: string (nullable = true)\n",
            " |-- AccountNumber: string (nullable = true)\n",
            " |-- rowguid: string (nullable = true)\n",
            "\n",
            "\n",
            "+----------+--------+-------+-----------+-------------+-------+\n",
            "|CustomerID|PersonID|StoreID|TerritoryID|AccountNumber|rowguid|\n",
            "+----------+--------+-------+-----------+-------------+-------+\n",
            "|     29485|   29485|  294.0|          4|   AW00029485|   null|\n",
            "|     29485|   29485|  294.0|          4|   AW00029485|   null|\n",
            "|     29486|   29486|  296.0|          3|   AW00029486|   null|\n",
            "|     29486|   29486|  296.0|          3|   AW00029486|   null|\n",
            "|     29486|   29486|  296.0|          3|   AW00029486|   null|\n",
            "|     29486|   29486|  296.0|          3|   AW00029486|   null|\n",
            "|     29486|   29486|  296.0|          3|   AW00029486|   null|\n",
            "|     29486|   29486|  296.0|          3|   AW00029486|   null|\n",
            "|     29486|   29486|  296.0|          3|   AW00029486|   null|\n",
            "|     29486|   29486|  296.0|          3|   AW00029486|   null|\n",
            "|     29486|   29486|  296.0|          3|   AW00029486|   null|\n",
            "|     29486|   29486|  296.0|          3|   AW00029486|   null|\n",
            "|     29487|   29487|  298.0|          2|   AW00029487|   null|\n",
            "|     29487|   29487|  298.0|          2|   AW00029487|   null|\n",
            "|     29487|   29487|  298.0|          2|   AW00029487|   null|\n",
            "|     29487|   29487|  298.0|          2|   AW00029487|   null|\n",
            "|     29487|   29487|  298.0|          2|   AW00029487|   null|\n",
            "|     29487|   29487|  298.0|          2|   AW00029487|   null|\n",
            "|     29487|   29487|  298.0|          2|   AW00029487|   null|\n",
            "|     29487|   29487|  298.0|          2|   AW00029487|   null|\n",
            "+----------+--------+-------+-----------+-------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}