{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3UeGMTfIBtq",
        "outputId": "61c3b804-963f-4e53-8959-665d27196801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=9123503b3eab89440adee30b9aca9f59df12dce87242bafb08cd4a9135914679\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5OG12jtLV0n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "P2e10I1KLsvL"
      },
      "outputs": [],
      "source": [
        "#spark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "sWGXtTrLL4QG"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"data_clean_person\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "HqEphSwDL8SD"
      },
      "outputs": [],
      "source": [
        "# Define the schema so it matches with the Big Query later\n",
        "schema = StructType([\n",
        "    StructField(\"BusinessEntityID\", IntegerType(), nullable=False),\n",
        "    StructField(\"PersonType\", StringType(), nullable=True),\n",
        "    StructField(\"NameStyle\", IntegerType(), nullable=True),\n",
        "    StructField(\"Title\", StringType(), nullable=True),\n",
        "    StructField(\"FirstName\", StringType(), nullable=True),\n",
        "    StructField(\"MiddleName\", StringType(), nullable=True),\n",
        "    StructField(\"LastName\", StringType(), nullable=True),\n",
        "    StructField(\"Suffix\", StringType(), nullable=True),\n",
        "    StructField(\"EmailPromotion\", IntegerType(), nullable=True),\n",
        "    StructField(\"AdditionalContactInfo\", StringType(), nullable=True),\n",
        "    StructField(\"Demographics\", StringType(), nullable=True),\n",
        "    StructField(\"rowguid\", StringType(), nullable=True),\n",
        "    StructField(\"ModifiedDate\", TimestampType(), nullable=True)\n",
        "])\n",
        "\n",
        "# Read the CSV file using the defined schema\n",
        "df = spark.read.csv('Person.Person.csv', sep=';', encoding='utf-8', header=True, schema=schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "b9buaOxiMDjw"
      },
      "outputs": [],
      "source": [
        "# Defining a UDF (User Defined Function)\n",
        "def clean_non_ascii(value):\n",
        "    if value is None:\n",
        "        return None\n",
        "    return value.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "clean_udf = udf(clean_non_ascii, StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8T5LBPqMiti",
        "outputId": "916cc244-24d4-46d8-d835-741fdfc5313f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+----------+---------+-----+---------+----------+----------+------+--------------+---------------------+--------------------+--------------------+-------------------+\n",
            "|BusinessEntityID|PersonType|NameStyle|Title|FirstName|MiddleName|  LastName|Suffix|EmailPromotion|AdditionalContactInfo|        Demographics|             rowguid|       ModifiedDate|\n",
            "+----------------+----------+---------+-----+---------+----------+----------+------+--------------+---------------------+--------------------+--------------------+-------------------+\n",
            "|               1|        EM|        0| NULL|      Ken|         J|    Snchez|  NULL|             0|                 NULL|\"<IndividualSurve...|92C4279F-1207-48A...|2009-01-07 00:00:00|\n",
            "|               2|        EM|        0| NULL|    Terri|       Lee|     Duffy|  NULL|             1|                 NULL|\"<IndividualSurve...|D8763459-8AA8-47C...|2008-01-24 00:00:00|\n",
            "|               3|        EM|        0| NULL|  Roberto|      NULL|Tamburello|  NULL|             0|                 NULL|\"<IndividualSurve...|E1A2555E-0828-434...|2007-11-04 00:00:00|\n",
            "|               4|        EM|        0| NULL|      Rob|      NULL|   Walters|  NULL|             0|                 NULL|\"<IndividualSurve...|F2D7CE06-38B3-435...|2007-11-28 00:00:00|\n",
            "|               5|        EM|        0|  Ms.|     Gail|         A|  Erickson|  NULL|             0|                 NULL|\"<IndividualSurve...|F3A3F6B4-AE3B-430...|2007-12-30 00:00:00|\n",
            "|               6|        EM|        0|  Mr.|   Jossef|         H|  Goldberg|  NULL|             0|                 NULL|\"<IndividualSurve...|0DEA28FD-EFFE-482...|2013-12-16 00:00:00|\n",
            "|               7|        EM|        0| NULL|    Dylan|         A|    Miller|  NULL|             2|                 NULL|\"<IndividualSurve...|C45E8AB8-01BE-4B7...|2009-02-01 00:00:00|\n",
            "|               8|        EM|        0| NULL|    Diane|         L|  Margheim|  NULL|             0|                 NULL|\"<IndividualSurve...|A948E590-4A56-45A...|2008-12-22 00:00:00|\n",
            "|               9|        EM|        0| NULL|     Gigi|         N|   Matthew|  NULL|             0|                 NULL|\"<IndividualSurve...|5FC28C0E-6D36-425...|2009-01-09 00:00:00|\n",
            "|              10|        EM|        0| NULL|  Michael|      NULL|    Raheem|  NULL|             2|                 NULL|\"<IndividualSurve...|CA2C740E-75B2-420...|2009-04-26 00:00:00|\n",
            "|              11|        EM|        0| NULL|   Ovidiu|         V|   Cracium|  NULL|             0|                 NULL|\"<IndividualSurve...|D2CC2577-EF6B-440...|2010-11-28 00:00:00|\n",
            "|              12|        EM|        0| NULL|  Thierry|         B|    D'Hers|  NULL|             2|                 NULL|\"<IndividualSurve...|FA263C7F-600D-4E8...|2007-12-04 00:00:00|\n",
            "|              13|        EM|        0|  Ms.|   Janice|         M|    Galvin|  NULL|             2|                 NULL|\"<IndividualSurve...|34EB99E0-7042-4DC...|2010-12-16 00:00:00|\n",
            "|              14|        EM|        0| NULL|  Michael|         I|  Sullivan|  NULL|             2|                 NULL|\"<IndividualSurve...|9A7501DE-5CAF-470...|2010-12-23 00:00:00|\n",
            "|              15|        EM|        0| NULL|   Sharon|         B| Salavaria|  NULL|             2|                 NULL|\"<IndividualSurve...|BEBA63CB-13F1-4B7...|2011-01-11 00:00:00|\n",
            "|              16|        EM|        0| NULL|    David|         M|   Bradley|  NULL|             1|                 NULL|\"<IndividualSurve...|2CC8BA72-5DBB-497...|2007-12-13 00:00:00|\n",
            "|              17|        EM|        0| NULL|    Kevin|         F|     Brown|  NULL|             2|                 NULL|\"<IndividualSurve...|9EE4713E-B3D8-440...|2007-01-19 00:00:00|\n",
            "|              18|        EM|        0| NULL|     John|         L|      Wood|  NULL|             2|                 NULL|\"<IndividualSurve...|FE21BDA7-9327-4D1...|2011-01-31 00:00:00|\n",
            "|              19|        EM|        0| NULL|     Mary|         A|   Dempsey|  NULL|             1|                 NULL|\"<IndividualSurve...|36F04305-6769-4E6...|2011-02-07 00:00:00|\n",
            "|              20|        EM|        0| NULL|   Wanida|         M|  Benshoof|  NULL|             2|                 NULL|\"<IndividualSurve...|1E7E56F4-A583-4E3...|2010-12-31 00:00:00|\n",
            "+----------------+----------+---------+-----+---------+----------+----------+------+--------------+---------------------+--------------------+--------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply the UDF to all string columns of the dataframe\n",
        "for col_name in df.columns:\n",
        "    if isinstance(df.schema[col_name].dataType, StringType):\n",
        "        df = df.withColumn(col_name, clean_udf(col_name))\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arYuHfesMtDP",
        "outputId": "12d3647a-c019-44fd-f507-23c692ef919d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- BusinessEntityID: integer (nullable = true)\n",
            " |-- PersonType: string (nullable = true)\n",
            " |-- NameStyle: integer (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- FirstName: string (nullable = true)\n",
            " |-- MiddleName: string (nullable = true)\n",
            " |-- LastName: string (nullable = true)\n",
            " |-- Suffix: string (nullable = true)\n",
            " |-- EmailPromotion: integer (nullable = true)\n",
            " |-- AdditionalContactInfo: string (nullable = true)\n",
            " |-- Demographics: string (nullable = true)\n",
            " |-- rowguid: string (nullable = true)\n",
            " |-- ModifiedDate: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "HkQb9butMn7c"
      },
      "outputs": [],
      "source": [
        "# Writing the transformed dataframe to a new CSV file\n",
        "df.coalesce(1).write.csv('table_person.csv', header=True, mode='overwrite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "I8aHVhQyNX7i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "tGx6rkSdMrKS"
      },
      "outputs": [],
      "source": [
        "# Stopping the Spark session\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
